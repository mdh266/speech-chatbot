{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2c2fd3a",
   "metadata": {},
   "source": [
    "# Building A Serverless Multimodal ChatBot: Part 1\n",
    "--------------------------------------------------\n",
    "\n",
    "__[1. Introduction](#first-bullet)__\n",
    "\n",
    "__[2. Chatting With Llama 3 Using LangChain & Groq](#second-bullet)__\n",
    "\n",
    "__[3. Speech & Text With Google Cloud API](#third-bullet)__\n",
    "\n",
    "__[4. Putting It Together As An App Using Streamlit](#fourth-bullet)__\n",
    "\n",
    "__[5. Next Steps](#fifth-bullet)__\n",
    "\n",
    "## 1. Introduction <a class=\"anchor\" id=\"first-bullet\"></a>\n",
    "---------------------\n",
    "\n",
    "In this blog post I will go over how to create a create multimodal chatbot using [Large Language Models (LLM)](https://en.wikipedia.org/wiki/Large_language_model). Specifically, we'll build an app that you can submit a prompt using speech and get the bot's reply back as speech. The conversation will be transcribed to text we can read it, but also so that we can interact with the LLM. I will go over how to do this all in a serverless framework and APIs so that (baring the app getting really popular) the costs will be next to nothing! We'll do this by using [LangChain](https://www.langchain.com/) & [Groq API](https://groq.com/) to interact with the [Llama 3](https://ai.meta.com/blog/meta-llama-3/) Open Source LLM. Then We'll use the Google Cloud API for [Text-To-Speech](https://cloud.google.com/text-to-speech?hl=en) and [Speech-To-Text](https://cloud.google.com/speech-to-text/?hl=en). For production and deployment we'll use [Streamlit](https://streamlit.io/), [Docker](https://www.docker.com/) and [Google Cloud Run](https://cloud.google.com/run).\n",
    "\n",
    "Lastly, I wanted to make this app so that my wife could practice Hebrew with and my mom could practice French with, so I made the app be able to be multilingual. In this post I'll focus on building the app and running it locally, while in a follow up one I will make a post on how to deploy the app.\n",
    "\n",
    "Now let's go over how to use LLMs!\n",
    "\n",
    "### 2. Chatting With Llama 3 Using LangChain & Groq <a class=\"anchor\" id=\"second-bullet\"></a>\n",
    "----------------------------\n",
    "\n",
    "There are many different [Large Language Models (LLM)](https://en.wikipedia.org/wiki/Large_language_model) that we can use for this app, but I chose [Llama 3](https://ai.meta.com/blog/meta-llama-3/) since its Open Source (free), specifically, I used the [Llama 3.3 70 Billion parameter model](https://groq.com/a-new-scaling-paradigm-metas-llama-3-3-70b-challenges-death-of-scaling-law/).\n",
    "\n",
    "For serving the model I used the [Groq API](https://groq.com/) since its free (up until a point). There are quite a few methods to interact with Groq and I chose to use [LangChain](https://www.langchain.com/). At first I thought LangChaing was a little over engineered (why do you need class for templated prompts? Isnt it just an f-string?), but now I am on-board! The API is super-powerful! It allows for a consistent API across most models and abstracts away a lot painpoints. The prompt templates do make sense now, and my only complaint is I cant tell what library something should come from (langchain, langchain_core, langchain_community?), but given ho much the API has changed around, it seems neither does the community. :-)\n",
    "\n",
    "The first thing I'll do is import [ChatGroq](https://python.langchain.com/docs/integrations/chat/groq/) class and use [pydot-env](https://pypi.org/project/python-dotenv/) to help with environment variables that hold my API keys.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "154a1441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924bf042",
   "metadata": {},
   "source": [
    "Instantiating the ChatGroq chat object gives me a model I can query using the `invoke` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4250bb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        temperature=0,\n",
    "        max_tokens=None,\n",
    "        timeout=None,\n",
    "        max_retries=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef25679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke(\"What is the square root of 9?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17af148f",
   "metadata": {},
   "source": [
    "The returned object is of type [AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) and the message can be obtained with the `.content` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36b4cba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The square root of 9 is 3.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d47adb",
   "metadata": {},
   "source": [
    "Simple enough! Now lets go over [PrompteTemplate](https://python.langchain.com/docs/concepts/prompt_templates/) in LangChain. PromptTemplates allow us to create prompts (question/queries) that can have variables in them (like [f-strings](https://realpython.com/python-f-strings/)). This allows us to chain together our prompt with the LLM into pipelines called \"Chains\" so that all we have to do is invoke the chain with a dictionary with the variable values and we will get back out answer for that invocation's prompt values!\n",
    "\n",
    "Let's show how TemplatePrompts work and how to use them with LLMs as a chain. First we import the PromptTemplate class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8beecc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e666c705",
   "metadata": {},
   "source": [
    "Next we create a string that looks like an `f-string`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e678e37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"What is the square root of {n}?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fd1a41",
   "metadata": {},
   "source": [
    "Now we use the [from_template](https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/prompts/prompt.py#L249) class method (pretty cool to see a class method, I have really not seen it used that often!) to make a templated prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64388789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['n'], input_types={}, partial_variables={}, template='What is the square root of {n}?')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b817f75d",
   "metadata": {},
   "source": [
    "Now we can create our prompt by filling in the variable `n` using a dictionary and the `invoke` method on the the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9eb852b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='What is the square root of 9?')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke({\"n\": 9})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1067dd83",
   "metadata": {},
   "source": [
    "Now the really cool thing is when we chain the PromptTemplate and the LLM together into a \"Chain\" using the `|` to represent seperate components of the chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8d6b7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0577c87a",
   "metadata": {},
   "source": [
    "We can go from value of n=16 to the answer now with just the invoke command using a dictionary as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fe33034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The square root of 16 is 4.\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({\"n\": 16})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bbcb85",
   "metadata": {},
   "source": [
    "Great!\n",
    "\n",
    "Now we can put it all together to create a function that takes a message in one language and converts it into another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0505cffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(language: str, text: str) -> str:\n",
    "        if language not in (\"English\", \"French\", \"Hebrew\"):\n",
    "                raise ValueError(f\"Not valid language choice: {language}\")\n",
    "        \n",
    "        template = \"Translate the following into {language} and only return the translated text: {text}\"\n",
    "\n",
    "        prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "        llm = ChatGroq(\n",
    "                model=\"llama-3.3-70b-versatile\",\n",
    "                temperature=0,\n",
    "                max_tokens=None,\n",
    "                timeout=None,\n",
    "                max_retries=2)\n",
    "\n",
    "        translation_chain = prompt | llm \n",
    "\n",
    "        result = translation_chain.invoke(\n",
    "                {\n",
    "                        \"language\": language,\n",
    "                        \"text\": text,\n",
    "                }\n",
    "        )\n",
    "\n",
    "        return result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bde157c",
   "metadata": {},
   "source": [
    "Now trying it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d901dd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = translate_text(language=\"French\", text=\"Hello World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e896b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour le monde !\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f010e483",
   "metadata": {},
   "source": [
    "Now one thing we have to do is add a bit a memory to our LLM since it wont remember anything we asked previous! See the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64337d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 9\n",
      "To determine the value of x + 3, I would need to know the value of x. Could you please provide the value of x?\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"Set x = 9\").content)\n",
    "print(llm.invoke(\"What is x + 3?\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eae4b8",
   "metadata": {},
   "source": [
    "The LLM has no recollection of anything from prior invocations! At first I thought memory was somthing special, but its really keeping track of the conversation and feeding it into the LLM before asking another question. The chat history will look like list of tuples where the first entry to the tuple signifies whether it is the \"ai\" system or the \"human\" and the second entry in the tuple is the message. For examplle the conversation above could be seen as,\n",
    "\n",
    "    history = [\n",
    "        (\"human\", \"Set x = 9\"),\n",
    "        (\"ai\", \"9\"),\n",
    "        (\"human\", \"What is x + 3?\"),\n",
    "        ...\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aa9d43",
   "metadata": {},
   "source": [
    "Similar to the [PrompteTemplate](https://python.langchain.com/docs/concepts/prompt_templates/) there is a [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) that can be used to create the history of the chat conversation. This used in conjunction with the [MessagePlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) to unwind the conversation into a prompt with the entire history and the new question at the very end. \n",
    "\n",
    "An examle is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7fe88f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        MessagesPlaceholder(\"history\"),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4450917",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [(\"human\", \"Set x = 9\"), (\"ai\", \"9\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c677e7cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Set x = 9', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='9', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is x + 3?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke(\n",
    "    {\n",
    "        \"history\": history,\n",
    "        \"question\": \"What is x + 3?\"\n",
    "    }\n",
    ").messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cf05ea",
   "metadata": {},
   "source": [
    "Now we can form a chain with memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e4652563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find x + 3, we need to add 3 to the current value of x, which is 9.\n",
      "\n",
      "x + 3 = 9 + 3\n",
      "x + 3 = 12\n",
      "\n",
      "So, x + 3 is 12.\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "chain = prompt | llm\n",
    "\n",
    "question = \"set x = 9\"\n",
    "answer = chain.invoke({\"history\": history, \"question\": question}).content\n",
    "history.extend([(\"human\", question), (\"ai\", answer)])\n",
    "\n",
    "question = \"what is x + 3?\"\n",
    "print(chain.invoke({\"history\": history, \"question\": question}).content)           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c76ad1e",
   "metadata": {},
   "source": [
    "Now I can put it all together into a function below using the history from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3d56d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, System \n",
    "from typing import Iterator, List, Tuple\n",
    "\n",
    "def ask_question(\n",
    "    history: List[Tuple[str, str]], \n",
    "    question: str,\n",
    "    ai_language: str\n",
    ") -> str:\n",
    "    \n",
    "    llm = ChatGroq(\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "            temperature=0,\n",
    "            max_tokens=None,\n",
    "            timeout=None,\n",
    "            max_retries=2)\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", f\"\"\"You are a helpful teacher having a conversation with a student in {ai_language}.\n",
    "             Only reply back in {ai_language} not matter what language the student uses.\"\"\"),\n",
    "            MessagesPlaceholder(\"history\"),\n",
    "            (\"human\", \"{question}\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm \n",
    "    \n",
    "    response = chain.invoke(\n",
    "                    {\n",
    "                        \"history\": history,\n",
    "                        \"question\": question\n",
    "                    }\n",
    "    )\n",
    "    \n",
    "    answer = response.content\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "459d3bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find the value of x + 3, we need to add 3 to the value of x. Since x = 9, we can calculate it as follows:\n",
      "\n",
      "x + 3 = 9 + 3\n",
      "= 12\n",
      "\n",
      "So, x + 3 is equal to 12.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    ask_question(\n",
    "    history=history,\n",
    "    ai_language=\"English\",\n",
    "    question=\"What is x + 3?\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6369bfdf",
   "metadata": {},
   "source": [
    "Now the prompt I set in prepending the history allows me to get the answer in any language! For instance,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d7c5e1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pour trouver la valeur de x + 3, il faut ajouter 3 à la valeur de x. Puisque x = 9, on a x + 3 = 9 + 3 = 12. La réponse est donc 12.\n"
     ]
    }
   ],
   "source": [
    "answer = ask_question(\n",
    "    history=history,\n",
    "    ai_language=\"French\",\n",
    "    question=\"What is x + 3?\")\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472011b9",
   "metadata": {},
   "source": [
    "Now in English! (The math in Hebrew got messed up... )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e0b44b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find the value of x + 3, you need to add 3 to the value of x. Since x = 9, we have x + 3 = 9 + 3 = 12. The answer is therefore 12.\n"
     ]
    }
   ],
   "source": [
    "print(translate_text(language=\"English\", text=answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2450dc4",
   "metadata": {},
   "source": [
    "Very cool!! \n",
    "\n",
    "LangChain makes this so easy! \n",
    "\n",
    "We have enough now to make a ChatBot, but I wanted to take this one step further and have an application you can speak with in one language and it would speak back to you in another (or the same) language.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52ddcca",
   "metadata": {},
   "source": [
    "### 3. Speech & Text With Google Cloud API <a class=\"anchor\" id=\"third-bullet\"></a>\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f49955b",
   "metadata": {},
   "source": [
    "In order to make an app that an end user can chat with using speech, we need to use [Speech-To-Text](https://cloud.google.com/speech-to-text?hl=en) to convert the end users audio into text that can be feed into `ask_question function above.\n",
    "\n",
    "The resulting response can be converted into an audio reply using [Text-To-Speech](https://cloud.google.com/text-to-speech?hl=en) and played back to the end users. There are  actually pretty straight forward using the Google Cloud API. I will just reference the code I wrote, [speech_to_text](../src/utils.py) and [text_to_speech](../src/utils.py) and note that there are [plently of languages](https://cloud.google.com/text-to-speech/docs/voices) that Google supports!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9567a50",
   "metadata": {},
   "source": [
    "### 4. Putting It Together As An App Using Streamlit <a class=\"anchor\" id=\"fourth-bullet\"></a>\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3cd06a",
   "metadata": {},
   "source": [
    "Now in order to make an app that people can interact with we need to create a front end. In the past I have done this more or less by hand, creating [Flask](http://michael-harmon.com/CrimeTime/) and [FastAPI](https://github.com/mdh266/TextClassificationApp). Nowdays many people use [Streamlit](https://streamlit.io/) to create the app which is *MUCH* easier!\n",
    "\n",
    "The Streamlit app is written module [main.py](../src/main.py). As I mentioned, in order to make LLM have memory I need to keep track of the conversation. I do so by using a list called `messages`. The way streamlit works is that it runs the entire script top to bottom any time anything in changed, so the messages would be cleared after run. In order to to maintain a history of the conversation were "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83158b59",
   "metadata": {},
   "source": [
    "### 5. Next Steps <a class=\"anchor\" id=\"fifth-bullet\"></a>\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42f7916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a377725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6012b3d4",
   "metadata": {},
   "source": [
    "For mapping to a website: https://www.youtube.com/watch?v=lDtvpUYAFzA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51df082",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
