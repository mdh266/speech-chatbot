{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2c2fd3a",
   "metadata": {},
   "source": [
    "# Building & Deploying A Serverless Multimodal ChatBot: Part 1\n",
    "--------------------------------------------------\n",
    "\n",
    "__[1. Introduction](#first-bullet)__\n",
    "\n",
    "__[2. Chatting With Llama 3 Using LangChain & Groq](#second-bullet)__\n",
    "\n",
    "__[3. Speech & Text With Google Cloud API](#third-bullet)__\n",
    "\n",
    "__[4. Putting It Together As An App Using Streamlit](#fourth-bullet)__\n",
    "\n",
    "__[5. Next Steps](#fifth-bullet)__\n",
    "\n",
    "## 1. Introduction <a class=\"anchor\" id=\"first-bullet\"></a>\n",
    "---------------------\n",
    "\n",
    "In this blog post I will go over how to create a create multimodal chatbot using a [Large Language Model (LLM)](https://en.wikipedia.org/wiki/Large_language_model). Specifically, I'll build an app that you can speak to and get an audio reply. The app will also optionally transcribe conversation. I will go over how to do this all in a serverless framework and using cloud-based APIs so that (baring the app getting really popular) the costs will be next to nothing! \n",
    "\n",
    "I'll do this by using [LangChain](https://www.langchain.com/) & [Groq API](https://groq.com/) to interact with the [Llama 3](https://ai.meta.com/blog/meta-llama-3/) Open Source LLM and the Google Cloud API for [Text-To-Speech](https://cloud.google.com/text-to-speech?hl=en) and [Speech-To-Text](https://cloud.google.com/speech-to-text/?hl=en). For the front end and deployment I'll use [Streamlit](https://streamlit.io/), [Docker](https://www.docker.com/) and [Google Cloud Run](https://cloud.google.com/run).\n",
    "\n",
    "Lastly, I wanted to make this app multi-lingual so that my wife could have someone to practice Hebrew with and my mom could practice French with. In this first post I'll cover building the app and running it locally, while in a follow up one I will cover how to deploy the app.\n",
    "\n",
    "Now let's go over how to use LLMs!\n",
    "\n",
    "### 2. Chatting With Llama 3 Using LangChain & Groq <a class=\"anchor\" id=\"second-bullet\"></a>\n",
    "----------------------------\n",
    "\n",
    "There are many different [Large Language Models (LLM)](https://en.wikipedia.org/wiki/Large_language_model) that we can use for this app, but I chose [Llama 3](https://ai.meta.com/blog/meta-llama-3/) since its Open Source (free); specifically, I used the [Llama 3.3 70 Billion parameter model](https://groq.com/a-new-scaling-paradigm-metas-llama-3-3-70b-challenges-death-of-scaling-law/).\n",
    "\n",
    "For serving the model I used the [Groq API](https://groq.com/) since its free for personal use (at least for me so far). There are quite a few methods to interact with Groq and I chose to use [LangChain](https://www.langchain.com/). At first I thought LangChain was a little over engineered (why do you need class for templated prompts? Isn't it just an f-string?), but now I see the point and am on-board! LangChain allows for a consistent API across most models and abstracts away a lot of pain points. The prompt templates do make sense now, and my only complaint is I cant tell what library something should come from (langchain, langchain_core, langchain_community?), but given how much the API has changed around, it seems neither does the community. :-)\n",
    "\n",
    "I'll start off going over how to use an LLM first. First thing I'll do is import [ChatGroq](https://python.langchain.com/docs/integrations/chat/groq/) class and use [pydot-env](https://pypi.org/project/python-dotenv/) to load environment variables that have my API keys.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "154a1441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924bf042",
   "metadata": {},
   "source": [
    "Instantiating the ChatGroq chat object gives me a model that I can query using the `invoke` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4250bb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        temperature=0,\n",
    "        max_tokens=None,\n",
    "        timeout=None,\n",
    "        max_retries=2)\n",
    "\n",
    "result = llm.invoke(\"What is the square root of 9?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17af148f",
   "metadata": {},
   "source": [
    "The returned object is of type [AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) and the message can be obtained with the `.content` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36b4cba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The square root of 9 is 3.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d47adb",
   "metadata": {},
   "source": [
    "Simple enough! \n",
    "\n",
    "Now let's go over using a [PrompteTemplate](https://python.langchain.com/docs/concepts/prompt_templates/) in LangChain. PromptTemplates are used to create prompts (questions/queries) that can have variables in them (like [f-strings](https://realpython.com/python-f-strings/)). This allows user to chain together the prompt with the LLM into a pipeline that is called a \"chain.\" Then the user only has to invoke the chain with an input dictionary that has the variables and their values and they will get back out the response for that user prompt!\n",
    "\n",
    "Let's show how TemplatePrompts work and how to use them with LLMs as a chain. First we import the PromptTemplate class and create a template string that looks sort of like like an `f-string`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8beecc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"What is the square root of {n}?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fd1a41",
   "metadata": {},
   "source": [
    "Now we use the [from_template](https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/prompts/prompt.py#L249) class method (I have not seen it them used that often!) to make a templated prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64388789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['n'], input_types={}, partial_variables={}, template='What is the square root of {n}?')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b817f75d",
   "metadata": {},
   "source": [
    "Now the actual prompt can be created by filling in the variable `n` using a dictionary from the `invoke` method,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9eb852b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='What is the square root of 9?')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke({\"n\": 9})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1067dd83",
   "metadata": {},
   "source": [
    "Now the really cool thing is when we chain the PromptTemplate and the LLM together into a \"chain\" using the `|` operator to represent seperate components of the chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8d6b7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0577c87a",
   "metadata": {},
   "source": [
    "This allows the user to input a value of n=16 using dictionary with a single invoke command and get back the reply!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fe33034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The square root of 16 is 4.\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({\"n\": 16})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bbcb85",
   "metadata": {},
   "source": [
    "Great!\n",
    "\n",
    "Now we can put it all together to create a function that takes a message in one language and converts it into another. I'll need this if the my end user speaks one lanuages and wants the bot to reply in another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0505cffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(language: str, text: str) -> str:\n",
    "        if language not in (\"English\", \"French\", \"Hebrew\"):\n",
    "                raise ValueError(f\"Not valid language choice: {language}\")\n",
    "        \n",
    "        template = \"Translate the following into {language} and only return the translated text: {text}\"\n",
    "\n",
    "        prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "        llm = ChatGroq(\n",
    "                model=\"llama-3.3-70b-versatile\",\n",
    "                temperature=0,\n",
    "                max_tokens=None,\n",
    "                timeout=None,\n",
    "                max_retries=2)\n",
    "\n",
    "        translation_chain = prompt | llm \n",
    "\n",
    "        result = translation_chain.invoke(\n",
    "                {\n",
    "                        \"language\": language,\n",
    "                        \"text\": text,\n",
    "                }\n",
    "        )\n",
    "\n",
    "        return result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bde157c",
   "metadata": {},
   "source": [
    "Notice I use the prompt to not only to pass in the users question, but also to tell the LLM to reply back in a specified language. Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d901dd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = translate_text(language=\"French\", text=\"Hello World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e896b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour le monde !\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f010e483",
   "metadata": {},
   "source": [
    "One issue with just using the LLM for chat bots is that it wont remember anything we asked previously! See the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64337d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 9\n",
      "To determine the value of x + 3, I would need to know the value of x. Could you please provide the value of x?\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"Set x = 9\").content)\n",
    "print(llm.invoke(\"What is x + 3?\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eae4b8",
   "metadata": {},
   "source": [
    "The LLM has no recollection of anything from prior invocations! \n",
    "\n",
    "We have add a \"memory\" our AI chatbot. At first I thought memory was something special, but its really keeping a record of the conversation and feeding the entire convesation so fare into the LLM before asking another question. The chat history will look like list of tuples. The first entry to the tuple signifies whether it is the \"ai\" system (chatbot) or the \"human\" and the second entry in the tuple is the actual message. For example the conversation above could be seen as,\n",
    "\n",
    "    history = [\n",
    "        (\"human\", \"Set x = 9\"),\n",
    "        (\"ai\", \"9\"),\n",
    "        (\"human\", \"What is x + 3?\"),\n",
    "        ...\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aa9d43",
   "metadata": {},
   "source": [
    "Similar to the [PrompteTemplate](https://python.langchain.com/docs/concepts/prompt_templates/) there is a [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) that can be used to create the history of the chat conversation. This used in conjunction with the [MessagePlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) to unwind the conversation into a prompt with the entire history and the new question at the very end. \n",
    "\n",
    "An example is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7fe88f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        MessagesPlaceholder(\"history\"),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4450917",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [(\"human\", \"Set x = 9\"), (\"ai\", \"9\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c677e7cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Set x = 9', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='9', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is x + 3?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke(\n",
    "    {\n",
    "        \"history\": history,\n",
    "        \"question\": \"What is x + 3?\"\n",
    "    }\n",
    ").messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cf05ea",
   "metadata": {},
   "source": [
    "Now we can form a chain with memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e4652563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find x + 3, we need to add 3 to the current value of x, which is 9.\n",
      "\n",
      "x + 3 = 9 + 3\n",
      "x + 3 = 12\n",
      "\n",
      "So, x + 3 is 12.\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "chain = prompt | llm\n",
    "\n",
    "question = \"set x = 9\"\n",
    "answer = chain.invoke({\"history\": history, \"question\": question}).content\n",
    "history.extend([(\"human\", question), (\"ai\", answer)])\n",
    "\n",
    "question = \"what is x + 3?\"\n",
    "print(chain.invoke({\"history\": history, \"question\": question}).content)           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c76ad1e",
   "metadata": {},
   "source": [
    "Putting it all together into a function below using the history concept from above,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3d56d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, System \n",
    "from typing import Iterator, List, Tuple\n",
    "\n",
    "def ask_question(\n",
    "    llm: ChatGroq,\n",
    "    history: List[Tuple[str, str]], \n",
    "    question: str,\n",
    "    ai_language: str,\n",
    ") -> str:\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", f\"\"\"You are a helpful teacher having a conversation with a student in {ai_language}.\n",
    "             Only reply back in {ai_language} not matter what language the student uses.\"\"\"),\n",
    "            MessagesPlaceholder(\"history\"),\n",
    "            (\"human\", \"{question}\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm \n",
    "    \n",
    "    response = chain.invoke(\n",
    "                    {\n",
    "                        \"history\": history,\n",
    "                        \"question\": question\n",
    "                    }\n",
    "    )\n",
    "    \n",
    "    answer = response.content\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "459d3bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find the value of x + 3, we need to add 3 to the value of x. Since x = 9, we can calculate it as follows:\n",
      "\n",
      "x + 3 = 9 + 3\n",
      "= 12\n",
      "\n",
      "So, x + 3 is equal to 12.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    ask_question(\n",
    "    llm=llm,\n",
    "    history=history,\n",
    "    ai_language=\"English\",\n",
    "    question=\"What is x + 3?\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6369bfdf",
   "metadata": {},
   "source": [
    "Now the prompt I set in prepending the history allows me to get the answer in any language! For instance,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d7c5e1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pour trouver la valeur de x + 3, il faut ajouter 3 à la valeur de x. Puisque x = 9, on a x + 3 = 9 + 3 = 12. La réponse est donc 12.\n"
     ]
    }
   ],
   "source": [
    "answer = ask_question(\n",
    "    llm=llm,\n",
    "    history=history,\n",
    "    ai_language=\"French\",\n",
    "    question=\"What is x + 3?\")\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472011b9",
   "metadata": {},
   "source": [
    "Now in English! (The math in Hebrew got messed up with sentences being read right to left... )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e0b44b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find the value of x + 3, you need to add 3 to the value of x. Since x = 9, we have x + 3 = 9 + 3 = 12. The answer is therefore 12.\n"
     ]
    }
   ],
   "source": [
    "print(translate_text(language=\"English\", text=answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2450dc4",
   "metadata": {},
   "source": [
    "Very cool!! \n",
    "\n",
    "LangChain makes this so easy! \n",
    "\n",
    "We now have enough to make a ChatBot, but I wanted to take this one step further and have an application you can speak with in one language and it would speak back to you in another (or the same) language.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52ddcca",
   "metadata": {},
   "source": [
    "### 3. Speech & Text With Google Cloud API <a class=\"anchor\" id=\"third-bullet\"></a>\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f49955b",
   "metadata": {},
   "source": [
    "In order to make an app that an end user can chat with using speech, we need to use [Speech-To-Text](https://cloud.google.com/speech-to-text?hl=en) to convert the end users audio into text that can be feed into the `ask_question` function above.\n",
    "\n",
    "The resulting response from the LLM can be converted into an audio reply using [Text-To-Speech](https://cloud.google.com/text-to-speech?hl=en) and played back to the end users. There are  actually pretty straight forward using the Google Cloud API. I will just reference the code I wrote, [speech_to_text](../src/utils.py) and [text_to_speech](../src/utils.py) and note that there are [plently of languages](https://cloud.google.com/text-to-speech/docs/voices) that Google supports!\n",
    "\n",
    "The one tricky part is setting up the API keys to be able to use these capabilities. The first step is to enable the [Speech-To-Text](https://cloud.google.com/speech-to-text?hl=en) and the [Text-To-Speech](https://cloud.google.com/text-to-speech?hl=en) services on your account. Next you will need to create an API key that you can use to access them. You can go to your console then select \"APIs & Services\" -> \"Enabled APIs & services\" as shown below:\n",
    "\n",
    "\n",
    "<img src=\"images/gcp_api_1.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "Then on the left sidebar select the \"Credentials\" tab, then on the top click \"Create Credentials\" and select \"API key\" from the drop down,\n",
    "\n",
    "![images/gcp_api_2.jpg](images/gcp_api_2.png)\n",
    "\n",
    " **Once you create your API key it will have unlimited access by default**, so let's restrict the access. You can click edit the API Key, and then under the \"API Restrictions\" section click \"Restrict Key\" and search for the \"Text-To-Speech\" and \"Speech-To-Test\" services,\n",
    "\n",
    "<img src=\"images/gcp_api_3.jpg\" alt=\"drawing\" width=\"500\"/>\n",
    " <!-- ![images/gcp_api_3.jpg](images/gcp_api_3.jpg) -->\n",
    "\n",
    " If you can't find the services in the search that probably means you didn't enable them in your account.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9567a50",
   "metadata": {},
   "source": [
    "### 4. Putting It Together As An App Using Streamlit <a class=\"anchor\" id=\"fourth-bullet\"></a>\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3cd06a",
   "metadata": {},
   "source": [
    "Now in order to make an app that people can interact with we need to create a front end. In the past I have done this more or less by hand using [Flask](http://michael-harmon.com/CrimeTime/) and [FastAPI](https://github.com/mdh266/TextClassificationApp). Nowdays many people use [Streamlit](https://streamlit.io/) to create apps which is *MUCH* easier!\n",
    "\n",
    "My Streamlit app is written module [main.py](../src/main.py) and uses the [audio_input](https://docs.streamlit.io/develop/api-reference/widgets/st.audio_input) function to capture the end users questions and uses the `speech_to_text` function to convert the audio to text. Before the question is sent to the `ask_question` function above I use the following function to convert the history of chats into list of tuples as shown above,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf613a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuplify(history: List[Dict[str, str]]) -> List[Tuple[str, str]]:\n",
    "    return [(d['role'], d['content']) for d in history]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34514d1",
   "metadata": {},
   "source": [
    "The LLM'S response is converted back to audio using the `text_to_speech` function and uses Streamlit's [audio](https://docs.streamlit.io/develop/api-reference/media/st.audio) function to play the response.\n",
    "\n",
    "As I mentioned, in order to make LLM have memory I need to keep track of the conversation. I do so by using a list called `messages`. The way Streamlit works is that it runs the entire script from top to bottom any time anything in changed, so the messages would be cleared after the first run. In order to to maintain a history of the conversation I had to save them part of the [session_state](https://docs.streamlit.io/develop/api-reference/caching-and-state/st.session_state). The last tricky part that I had to figure out was how to create a button to clear the messages. Every time I tried, it still had the variable from the `audio_input` set and I couldnt clear the first message. In order to fix this I had to create a [form](https://docs.streamlit.io/develop/concepts/architecture/forms) along with using the [form_submit_button](https://docs.streamlit.io/develop/api-reference/execution-flow/st.form_submit_button) and viola the clear button now worked!\n",
    "\n",
    "You can try running the app using the command,\n",
    "\n",
    "    streamlit run src/main.py \n",
    "\n",
    "If your browser doesn't automatically open, you can go to https://localhost:8051"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83158b59",
   "metadata": {},
   "source": [
    "### 5. Next Steps <a class=\"anchor\" id=\"fifth-bullet\"></a>\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42f7916",
   "metadata": {},
   "source": [
    "In the next post I'll cover how to deploy this app using [Docker](https://www.docker.com/) for containerization which will allow us to run the app both locally and on the cloud. Then well cover [GitHub Actions](https://github.com/features/actions) for automatically building the image and pushing it to [Docker Hub](https://hub.docker.com/) where it can be pulled and run on [Google Cloud Run](https://cloud.google.com/run) to create a serverless application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
